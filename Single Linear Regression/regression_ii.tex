\documentclass[notheorems, 9pt]{beamer}

% Packages with options
\usepackage[english]{babel}
\usepackage[mathscr]{euscript}
\usepackage[utf8]{inputenc}

% Primary Packages
\usepackage{amsbsy, amsmath, amssymb, amsthm, bm, commath, chngcntr, dsfont, econometrics, gensymb, graphicx, IEEEtrantools, longtable, marginnote, mathrsfs, mathtools, mdframed, natbib, parskip, pgf, setspace, subfigure, tabularx, textcomp, tikz}

% Rest of the setup is in the "setup_beamer" package
\usepackage{setup_beamer}

% Title, Author, Institute
\title{Econ 103: Topics in Single Linear Regression}
\author{Manu Navjeevan}
\institute{UCLA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\frame{\titlepage}

%NOTE: Cover: R^2, Inference on Linear Combinations, Forecast Error, Heteroskedasticity

\begin{frame}{Content Outline} 
	\label{frame:content-outline}
	\ucla{Advanced Inference Topics}
	\begin{itemize}
		\item Inference on Linear Combinations of Parameters
		\item Heteroskedasticity
	\end{itemize}
	\ucla{Evaluating our Model}
	\begin{itemize}
		\item \(R^2\) and goodness of fit
	\end{itemize}
	\ucla{Modeling Choices}
	\begin{itemize}
		\item How do results change if we apply linear transformations?
		\item Useful non-linear transformations of \(X\) and \(Y\)
	\end{itemize}
\end{frame}

\section{Advanced Inference Topics}
%NOTE: Linear combinations, forecast error, heteroskedasticity
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc1}
	Recall that, approximately for large \(n\)
	\begin{align*}
		\sqrt{n}(\ucla{\hat\beta_0} - \ucla{\beta_0}) \sim N\left(0, \E[X^2]\sigma_\eps^2/\sigma_X^2 \right),\;\;
		\sqrt{n}(\ucla{\hat\beta_1} - \ucla{\beta_1}) \sim N\left(0, \sigma_\eps^2/\sigma_X^2\right)
	\end{align*}
	and \(\sigma_{\beta_{01}} = \Cov(\sqrt{n}\{\ucla{\hat\beta_0}-\ucla{\beta_0}\},\sqrt{n}\{\ucla{\hat\beta_1}-\ucla{\beta_1}\}) = -\E[X]\frac{\sigma_\eps^2}{\sigma_X^2}.\)
	\onslide<2->
	\vspace{0.5cm}

	These results were also often presented in the following equivalent manners
	\begin{align*}
		\frac{\ucla{\hat\beta_0}-\ucla{\beta_0}}{\sigma_{\beta_0}/\sqrt{n}}\sim N(0,1) &\andbox \ucla{\hat\beta_0} \sim N(\ucla{\beta_1},\sigma_{\beta_0}^2/n) \\
		\frac{\ucla{\hat\beta_1}-\ucla{\beta_1}}{\sigma_{\beta_1}/\sqrt{n}}\sim N(0,1) &\andbox \ucla{\hat\beta_1}\sim N(\ucla{\beta_1},\sigma_{\beta_1}^2/n) 
	\end{align*}
	where \(\sigma_{\beta_0}^2 = \E[X^2]\sigma_{\eps^2}/\sigma_X^2\) and \(\sigma_{\beta_1}^2 = \sigma_\eps^2/\sigma_X^2\).
	 \begin{itemize}
		\item<3-> Also went over how to estiamte these variances
	\end{itemize}
	
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc}
	In the last lecture, we used these distributional results to compute objects like 
	\[
		\Pr\left(|\ucla{\hat\beta_1}| > 5 \mid \ucla{\beta_1} = 0\right)
	.\] 
	which in turn were useful for hypothesis testing
	\[
		\green{H_0}:\ucla{\beta_1}= 0\vsbox \red{H_1}:\ucla{\beta_1} \neq 0
	.\] 
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc2}
	However, often we want to preform inference not just on one parameter, but on a linear combination of parameters, i.e we want to test 
	\[
		\green{H_0}:\ucla{\beta_0} + 5\ucla{\beta_1} = 0\vsbox \red{H_1}:\ucla{\beta_0} + 5\ucla{\beta_1}\neq 0
	.\] 
	\onslide<2->
	This is useful, for example, if we are trying to test something like 
	\[
		\green{H_0}:\E[Y|X=5] = 0\vsbox\red{H_1}:\E[Y|X=5]\neq 0
	\] 
	and we view the linear regression model \(Y = \ucla{\beta_0} + \ucla{\beta_1}X + \eps\) as a way of approximating the conditional mean function \(\E[Y|X=x]\). 
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc3}
	In order to test such a hypothesis we want to know the distribution of a linear combination of our model parameters. That is, for \(\lambda = a\ucla{\beta_0} + b\ucla{\beta_1}\) we would like to know the approximate distribution of
	\[
	    \hat\lambda = a\ucla{\hat\beta_0} + b \ucla{\hat\beta_1}
	\] 
	so that we can calculate objects like \(\Pr(|\hat\lambda| > 0.5 \mid \lambda = 0)\).
	\onslide<2->

	Note that
	\begin{align*}
		\sqrt{n}\left(\hat\lambda - \lambda\right) &= \sqrt{n}\left(a\ucla{\hat\beta_0} + b\ucla{\hat\beta_1} - a\ucla{\beta_0} - b \ucla{\beta_1}\right) \\
		&= a\sqrt{n}\left(\ucla{\hat\beta_0} - \ucla{\beta_0}\right) + b\sqrt{n}\left(\ucla{\hat\beta_1}-\ucla{\beta_1}\right)
	.\end{align*} 
	and that we know the (joint) distribution of \( \sqrt{n}(\ucla{\hat\beta_0}-\ucla{\beta_0})\) and \(\sqrt{n}(\ucla{\hat\beta_1}-\ucla{\beta_1})\).
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc4}
	Recall from our Econ 41 Review that the sum of two jointly normal random variables is also normally distributed and that if \(X\) and  \(Y\) are random variables then 
	 \[
		 \Var(aX + bY) = a^2\Var(X) + b^2\Var(Y) + 2ab\Cov(X,Y)
	.\] 
	\onslide<2->
	Using this result along with \(X = \sqrt{n}(\ucla{\hat\beta_0} - \ucla{\beta_0})\) and \(Y = \sqrt{n}(\ucla{\hat\beta_1}-\ucla{\beta_1})\) gives us that, for large \(n\):
	\[
		\sqrt{n}\left(\hat\lambda - \lambda\right) \sim N(0,\sigma_\lambda^2) \implies \frac{\hat\lambda-\lambda}{\sigma_\lambda/\sqrt{n}}\sim N(0,1) 
	,\]
	where \(\sigma_\lambda^2 = a^2\sigma_{\beta_0}^2 + b^2\sigma_{\beta_1}^2 + 2ab\sigma_{\beta_{01}}\)	
\end{frame}
	
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc6}
	As a reminder, we can estimate 
	\begin{align*}
		\sigma_{\beta_0}^2 = \E[X^2]\frac{\sigma_\eps^2}{\sigma_X^2} &\iff \hat\sigma_{\beta_0}^2 = \frac{1}{n}\sum_{i=1}^n X_i^2\cdot \frac{\hat\sigma_\eps^2}{\hat\sigma_X^2}  \\
		\sigma_{\beta_1}^2 = \frac{\sigma_\eps^2}{\sigma_X^2} &\iff \hat\sigma_{\beta_1}^2 = \frac{\hat\sigma_\eps^2}{\hat\sigma_X^2}   \\
		\sigma_{\beta_{01}} = -\E[X]\frac{\sigma_\eps^2}{\sigma_X^2} &\iff\hat\sigma_{\beta_{01}} = \bar X\frac{\hat\sigma_\eps^2}{\hat\sigma_X^2} 
	\end{align*}
	\onslide<2->
	So, we can use these to estimate \(\sigma_\lambda^2 = a^2\sigma_{\beta_0}^2 + b^2\sigma_{\beta_1}^2 + 2ab\sigma_{\beta_{01}}\) with
	\[
		\hat\sigma_\lambda^2 = a^2\hat\sigma_{\beta_0}^2 + b^2\hat\sigma_{\beta_1}^2 + 2ab\hat\sigma_{\beta_{01}}
	.\] 
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc7}
	As \(n\to \infty\), \(\hat\sigma_{\beta_0}^2 \to \sigma_{\beta_0}^2\),  \(\hat\sigma_{\beta_1}^2 \to \sigma_{\beta_1}^2\), and \(\hat\sigma_{\beta_{01}}\to \sigma_{\beta_{01}}\) by the \red{Law of Large Numbers}. This gives us that \(\hat\sigma_{\lambda}^2 \to \sigma_{\lambda}^2\) as  \(n\to \infty\) so that we can say (approximately for large  \(n\)):
	 \[
		 \frac{\hat\lambda - \lambda}{\hat\sigma_\lambda/\sqrt{n}}\sim N(0,1) 
	.\]
	\onslide<2->
	As when considering just \(\ucla{\hat\beta_0}\) or \(\ucla{\hat\beta_1}\), this distributional result will be useful for hypothesis testing and creating confidence intervals.
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc7.5}
	Using the distributional result:
	\[
		\frac{\hat\lambda-\lambda}{\hat\sigma_\lambda/\sqrt{n}}\sim N(0,1) 
	,\] 
	we can test a null hypothesis of the form \(\green{H_0}:\lambda \leq \ell\), \(\green{H_0}:\lambda \geq \ell\), or \(\green{H_0}:\lambda =\ell\) by first constructing our test statistic
	 \[
	    t^* = \frac{\hat\lambda - \ell}{\hat\sigma_\lambda/\sqrt{n}} 
	.\]
	\onslide<2->
	As before, we want to reject our null hypothesis if the probability of obtaining our test statistic (or something even further from the null hypothesis) under the null hyptohesis is less than or equal to some pre-specified value \(\lambda\).
	\onslide<3->
	\begin{itemize}
		 \item Recall that by the distributional result, under the null \(t^* \sim N(0,1)\)
		 \item The quantity \(\hat\sigma_\lambda/\sqrt{n}\) is called the \red{standard error} of \(\hat\lambda\).
	\end{itemize}
\end{frame}
%TODO: Consider adding a homework question about test consistency
%TODO: Add a homework question getting students to think about whether a linear model is good
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc7.75}
	Now that we have constructed our test statistic \(t^*\) we can conduct our test in two (equivalent) ways, as before
	 \begin{enumerate}
		\item<1|only@1> Construct a p-value and reject if \(p<\alpha\):
		 \begin{itemize}
			 \item If \(\green{H_0}:\lambda \leq \ell\) and \(\red{H_1}:\lambda > \ell\):
				 \[
					 p = \Pr(Z \geq  t^*)
				 .\] 
			 \item If \(\green{H_0}:\lambda \geq \ell\) and \(\red{H_1}:\lambda < \ell\):
				  \[
					  p = \Pr(Z \leq  t^*)
				 .\] 
			 \item If \(\green{H_0}:\lambda = \ell\) and  \(\red{H_1}:\lambda \neq \ell\):
				  \[
					  p = \Pr(|Z| \geq  |t^*|) = 2\Pr(Z \geq |t^*|)
				 .\]
		\end{itemize}
	\item<2|only@2> Compare the t statistic to the \(1-\alpha\) or  \(1-\alpha/2\) quantile of the standard normal distribution:  \(z_{1-\alpha}\) or  \(z_{1-\alpha/2}\).
		 \begin{itemize}
			 \item If \(\green{H_0}:\lambda \leq \ell\) and \(\red{H_1}:\lambda > \ell\) reject if 
				 \[
					 t^* \geq z_{1-\alpha}
				 .\] 
			 \item If \(\green{H_0}:\lambda \geq \ell\) and \(\red{H_1}:\lambda < \ell\) reject if
				  \[
					  t^* \leq -z_{1-\alpha}
				 .\] 
			 \item If \(\green{H_0}:\lambda = \ell\) and  \(\red{H_1}:\lambda \neq \ell\) reject if
				  \[
					  |t^*| \geq z_{1-\alpha/2}
				 .\]
		\end{itemize}
		As a reminder \(z_{1-\alpha}\) and  \(z_{1-\alpha/2}\) are such that 
		\begin{align*}
			\Pr(Z \leq z_{1-\alpha}) = 1-\alpha &\iff \Pr(Z > z_{1-\alpha}) = \alpha \\ 
			\Pr(Z \leq z_{1-\alpha/2}) = 1-\alpha/2 &\iff \Pr(|Z| > z_{1-\alpha/2}) = \alpha
		\end{align*}
		%TODO: Homework: assign showing this second equality
	\end{enumerate}
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc7.8}
	We can also construct a \(100(1-\alpha)\%\) confidence interval for \(\lambda\) in the same way as before: by looking at the values of \(\ell\) for which we would \green{fail to reject} the null hypothesis \(\green{H_0}:\lambda = \ell\) against a two-sided alternative  \(\red{H_1}:\lambda\neq \ell\) at level \(\alpha\).
	\onslide<2->
	
	This gives us  a symmetric formula as before, a \(100(1-\alpha)\%\) confidence interval for  \(\lambda\) is given
	 \[
		 \hat\lambda \pm z_{1-\alpha/2}\frac{\hat\sigma_\lambda}{\sqrt{n}} 
	.\] 
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:aside}
	As an aside, we can start to see a pattern here. Essentially anytime we have a distributional result like 
	\[
		\frac{\text{Estimator} - \text{True Value}}{\text{Standard Error of Estimator}} \sim N(0,1)
	.\] 
	we can test a null hypothesis by constructing our test statistic
	\[
		t^* = \frac{\text{Estimator} - \text{Null Hypothesis Value}}{\text{Standard Error of Estimate}} 
	.\]
	and then computing a \(p\)-value or directly compating this test statistic to  \(z_{1-\alpha}\),  \(-z_{1-\alpha}\), or  \(z_{1-\alpha/2}\) (depending on what alternate hypothesis we are testing).
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:aside2}
	We can also use this distributional result to generate \(100(1-\alpha)\%\) confidence intervals for the true value via
	 \[
		 \text{Estimator} \pm z_{1-\alpha/2}\cdot\text{Standard Error of Estimator}
	.\] 
\end{frame}
\begin{frame}{Linear Combinations of Parameters: Questions}
	\centering
	\red{\Large Questions?}
\end{frame} 
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc8}
	\ucla{Example:} Suppose we are arguing with our professional colleague Kyle Kuzma about the relationship between number of mental health days taken in a month (\(X\)) and the average number of points per game scored in the NBA (\(Y\)). Kuzma claims that \(\E[Y|X=3] = 20\), we want to test this claim at level \(\alpha = 0.05\).
	\onslide<2->
	
	First we collect a random sample of 49 NBA players and ask them how many mental health days they took this month and their average points per game, \(\{Y_i,X_i\}_{i=1}^{49}\). 	Then, since we believe the relationship between \(Y\) and  \(X\) to be linear, we estimate the linear model 
	 \[
		 Y = \ucla{\beta_0} + \ucla{\beta_1}\cdot X + \eps
	.\] 
	\onslide<3->
	We can then estimate \(\E[Y|X=3]\) by  \(\ucla{\hat\beta_0} + 3\ucla{\hat\beta_1}\).
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc9}
	We can test Kuzma's claim that \(\E[Y|X=3] = 20\) by running the following hypothesis test
	\[
		\green{H_0}:\ucla{\beta_0} + 3\ucla{\beta_1} = 20 \vsbox\red{H_1}:\ucla{\beta_0} + 3\ucla{\beta_1}\neq 20
	.\] 
	\onslide<2->
	To test this claim we use our data (with \(n=49\)) to estimate
	\begin{align*}
		\ucla{\hat\beta_0} = 10,\;&\;\;\;  \ucla{\hat\beta_1} = 3 \\ \hat\sigma_{\beta_0}^2 = \hat\sigma_{\beta_0}^2 &= \hat\sigma_{\beta_{01}} = 1
	\end{align*}
	\onslide<3->
	Using these estimates we get
	\begin{align*}
		\hat\lambda &= \ucla{\hat\beta_0} + 3\ucla{\hat\beta_1} = 19 \\
		\hat\sigma_{\lambda}^2 &= \hat\sigma_{\beta_0}^2 + 9\hat\sigma_{\beta_1}^2 + 6\hat\sigma_{\beta_{01}} = 16
	\end{align*}
	\onslide<4->
	\begin{itemize}
		\item Notice how much larger \(\hat\sigma_\lambda^2\) is than  \(\hat\sigma_{\beta_0}^2\) or \(\hat\sigma_{\beta_1}^2\).
	\end{itemize}
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc10}
	Using \(\hat\lambda = 19\), \(\hat\sigma_{\lambda}^2 = 16\), and \(n = 49\) we can construct our test statistic for \(\green{H_0}:\lambda = 20\) vs  \(\red{H_1}:\lambda \neq 20\)
	 \[
	    t^* = \frac{\hat\lambda - 20}{\hat\sigma_\lambda/\sqrt{n}} = \frac{19-20}{\sqrt{16}/\sqrt{49}} = -\frac{1}{4/7} = -\frac{7}{4} = -1.75    
	.\]
	\only<2>{
	We'll run our test in two ways. First, let's compute our p-value
	\[
		p = \Pr(|Z| \geq |-1.75|) = 2\Pr(Z \geq 1.75) = 2(1-\Pr(Z\leq 1.75)) = 2\cdot 0.04 = 0.08
	.\]
	Since \(0.08 > 0.05\) we \green{fail to reject} Kuzma's claim.}
	
	\only<3>{
	We'll run our test in two ways. Next, let's compare our test statistic to \(z_{1-\alpha/2}\). Since \(\alpha =2\) we get that  \(z_{1-\alpha/2} = z_{0.975}= 1.96\). Because
	\[
		|t^*| = 1.75 < 1.96 = z_{0.975} 
	\] 
	we again \green{fail to reject} Kuzma's claim}
\end{frame}
\begin{frame}{Inference: Linear Combinations of Parameters} 
	\label{frame:lc11}
	Let's use these same estimates, \(\hat\lambda = 19\) and \(\hat\sigma_{\lambda}^2 = 16\), to construct a \(95\%\) confidence interval for the true parameter  \(\lambda = \ucla{\beta_0} + 3\ucla{\beta_1}\).
	\begin{itemize}
		\item<2|only@2> Since we have assumed that the true relationship between \(Y\) (points per game) and  \(X\) (number of mental health days taken per month) is linear then  \(\lambda = \E[Y|X=3]\).
		\begin{itemize}
			\item By linear we mean that \(\E[Y|X=x] = \ucla{\beta_0} + \ucla{\beta_1}\cdot x\)
			\item Otherwise we can view \(\lambda = \ucla{\beta_0} + 3\ucla{\beta_1}\) as an approximation of \(\E[Y|X=3]\).
		\end{itemize}
	\end{itemize}
	\onslide<3->

	From above we have that a \(95\%\) confidence interval for  \(\lambda\) can be constructed
	 \[
		 \hat\lambda \pm z_{0.975}\frac{\hat\sigma_\lambda}{\sqrt{n}} = 19 \pm 1.96\frac{7}{4}  
	.\] 
\end{frame}

\section{Evaluating our Model}
%NOTE: R^2 

\section{Modeling Choices}%
%NOTE: Stability under linear transformations, nonlinear trasformations/elasticity


\end{document}


