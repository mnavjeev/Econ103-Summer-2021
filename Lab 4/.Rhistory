knitr::opts_chunk$set(echo = TRUE)
# Clear the workspace
rm(list = ls())
# Our data set is store as part of the PoEData library, so let's tell R we need access to it
library(PoEdata)
# Looking at the output, there are multiple datasets called CPS. We'll use cps.
data(cps)
# Remind ourselves what variables are contained in cps dataset
attributes(cps)
(16 < 18)
(16 < 15)
1*(16 < 18)
1*(16 < 15)
test <- c(15, 16, 17, 18)
(test < 16)
1*(test < 16)
print(cps$educ[1:10])
# (cps$educ >= 16) returns a vector that has a TRUE entry if education is larger than 16 and a FALSE entry otherwise
print((cps$educ >= 16)[1:10])
#  Create a log wages variable
logwage <- log(cps$wage)
# Run the desired regression
regout <- lm(logwage ~ educ + D, data = cps)
#  Create a log wages variable
logwage <- log(cps$wage)
# Run the desired regression
regout <- lm(logwage ~ educ + D, data = cps)
# If we multiply this by one, we will get the dummy variable that we want
D <- 1*(cps$educ >= 16)
#  Create a log wages variable
logwage <- log(cps$wage)
# Run the desired regression
regout <- lm(logwage ~ educ + D, data = cps)
# Obtain summary and print it out
regsum <- summary(regout)
regsum
# Include our new dummy variable
intregout <- lm(logwage ~ educ + female + I(female*educ), data = cps)
# Obtain summary and print it out
intregsum <- summary(intregout)
intregsum
# Extract the relevant coefficients
b2 <- intregsum$coefficients[3,1]
b3 <- intregsum$coefficients[4,1]
# Plot the curve for education between 0 and 20
curve(-b2 - b3*x , from = 0, to = 20, xlab = "Years of Education", ylab ="Expected Gap (logMen - logWomen)", ylim = c(0,0.6))
attributes(intregsum)
intregsum$coefficients
# Clear the workspace
rm(list = ls())
# Our data set is store as part of the PoEData library, so let's tell R we need access to it
library(PoEdata)
# Looking at the output, there are multiple datasets called CPS. We'll use cps.
data(cps)
# We'll begin by creating log wages for our regression
logwage <- log(cps$wage)
# Next we'll run the unrestricted regression
Uregout <- lm(logwage ~ educ + exper + I(exper^2), data = cps)
# And the restricted regression
Rregout <- lm(logwage ~ educ, data = cps)
attributes(Rregout)
Rregout$residuals
# The residuals are being stored under Rregout$residuals, which lets us get the sum of squares
SSER <- sum(Rregout$residuals^2)  # Get the restricted sum of squares
Uregout$residuals^2
SSEU <- sum(Uregout$residuals^2)  # Get the unrestricted sum of squares
# Now compute the F statistic
F <- ((SSER - SSEU)/2)/(SSEU/df)
# Examine the output of the `lm' function
attributes(Rregout)
# The residuals are being stored under Rregout$residuals, which lets us get the sum of squares
SSER <- sum(Rregout$residuals^2)  # Get the restricted sum of squares
SSEU <- sum(Uregout$residuals^2)  # Get the unrestricted sum of squares
# The next thing we need is J and n-p-1. J = 2, and n-p-1 is just the "degrees of freedom" of unrestricted regression. We can access this through the output of our regression via the df.residual attribute. Or by substracting 4 from the number of rows in our cps dataset.
df <- Uregout$df.residual
#df <- nrow(cps) - 4
# Now compute the F statistic
F <- ((SSER - SSEU)/2)/(SSEU/df)
# Recall we stored (n-p-1) in the value df
# Call pf to get the p-value for this null hypothesis
1-pf(F,2,df)
# Get the summary for the full unrestricted model
summary(Uregout)
# Let's begin by clearing the workspace and loading the cps data
# Clear the workspace
rm(list = ls())
# Our data set is store as part of the PoEData library, so let's tell R we need access to it
library(PoEdata)
# Looking at the output, there are multiple datasets called CPS. We'll use cps.
data(cps)
# Then run the regression we are interested in
# Create log wages
logwage <- cps$wage
# Run the regression we need
regout <- lm(logwage ~ educ + exper + I(exper^2), data = cps)
# Process out
regsum <- summary(regout)
regsum$coefficients
# First let's get the coefficients. If unsure it is helpful to print out output
# to make sure we are assigning the right variables
regsum$coefficients
# We see b2 and b3 are in the third and fourth columns
b2 <- regsum$coefficients[3,1]
# Create out estimate theta
theta <- b2 + 59*b3
b3 <- regsum$coefficients[4,1]
# Create out estimate theta
theta <- b2 + 59*b3
# Next to get the covariance matrix us vcov
covm <- vcov(regout)
# Print it out to see what it looks like
covm
# Now estimate the var of theta.
# Recall that the intercept is the first term, so that beta hat 2 is the *third* row/column
vartheta <- covm[3,3] + (59^2)*covm[4,4] + 2*59*covm[3,4]
# We are now ready to obtain the pvalue
pval <- (1-pnorm(theta/sqrt(vartheta)))
# Print it out
pval
# First let's get the coefficients. If unsure it is helpful to print out output
# to make sure we are assigning the right variables
regsum$coefficients
# We see b2 and b3 are in the third and fourth columns
b2 <- regsum$coefficients[3,1]
b3 <- regsum$coefficients[4,1]
# Create out estimate theta
theta <- b2 + 59*b3
# Next to get the covariance matrix us vcov
covm <- vcov(regout)
# Print it out to see what it looks like
covm
# Now estimate the var of theta.
# Recall that the intercept is the first term, so that beta hat 2 is the *third* row/column
vartheta <- covm[3,3] + (59^2)*covm[4,4] + 2*59*covm[3,4]
# We are now ready to obtain the pvalue
pval <- (1-pnorm(theta/sqrt(vartheta)))
# Print it out
pval
